import streamlit as st
from datetime import date
from ingest import load_file, load_url, split_documents, crawl_website, search_google_news
from rag_chain import (
    add_documents_to_vectorstore,
    get_document_count,
    clear_vectorstore,
    clear_vectorstore_by_type,
    get_document_counts_by_type,
    get_rag_response_with_sources,
    migrate_file_types
)

st.set_page_config(
    page_title="RAG Knowledge Base",
    page_icon="ğŸ“š",
    layout="wide"
)

if "migrated" not in st.session_state:
    migrate_file_types()
    st.session_state.migrated = True

import streamlit.components.v1 as components
components.html("""
<script>
(function() {
    var pdoc = window.parent.document;
    var existing = pdoc.getElementById('chat-float-container');
    if (existing) existing.remove();
    var container = pdoc.createElement('div');
    container.id = 'chat-float-container';
    container.innerHTML = '<div style="position:fixed;bottom:28px;right:28px;z-index:999999;display:flex;flex-direction:column;align-items:center;gap:8px;">' +
        '<div style="background:#1E293B;color:#fff;padding:5px 14px;border-radius:8px;font-size:12px;font-weight:700;white-space:nowrap;box-shadow:0 2px 10px rgba(0,0,0,0.3);letter-spacing:0.5px;">AI CHATBOT</div>' +
        '<button id="chat-float-btn" style="width:64px;height:64px;border-radius:50%;border:none;background:linear-gradient(135deg,#3B82F6,#2563EB);color:#fff;font-size:28px;cursor:pointer;box-shadow:0 4px 20px rgba(59,130,246,0.5);transition:transform 0.2s,box-shadow 0.2s;display:flex;align-items:center;justify-content:center;line-height:1;">&#x1F4AC;</button>' +
        '</div>';
    pdoc.body.appendChild(container);
    var btn = pdoc.getElementById('chat-float-btn');
    btn.addEventListener('mouseenter', function() {
        this.style.transform = 'scale(1.1)';
        this.style.boxShadow = '0 6px 28px rgba(59,130,246,0.65)';
    });
    btn.addEventListener('mouseleave', function() {
        this.style.transform = 'scale(1)';
        this.style.boxShadow = '0 4px 20px rgba(59,130,246,0.5)';
    });
    btn.addEventListener('click', function() {
        var w = 390, h = 620;
        var left = (screen.width - w - 40);
        var top2 = (screen.height - h - 80);
        window.parent.open(
            window.parent.location.origin + '/chat_widget',
            'AI_CHATBOT',
            'width=' + w + ',height=' + h + ',left=' + left + ',top=' + top2 + ',resizable=yes,scrollbars=yes'
        );
    });
})();
</script>
""", height=0)

if "messages" not in st.session_state:
    st.session_state.messages = []

if "sources" not in st.session_state:
    st.session_state.sources = []


def get_api_key():
    import os
    if os.environ.get("GOOGLE_API_KEY"):
        return os.environ.get("GOOGLE_API_KEY")
    if "GOOGLE_API_KEY" in st.secrets:
        return st.secrets["GOOGLE_API_KEY"]
    return None


def main():
    st.title("ğŸ“š RAG ì§€ì‹ ê¸°ë°˜")
    st.markdown("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ URLì„ ì¶”ê°€í•˜ì—¬ ì§€ì‹ ê¸°ë°˜ì„ êµ¬ì¶•í•˜ê³ , ë°ì´í„°ì™€ ëŒ€í™”í•˜ì„¸ìš”!")
    
    api_key = get_api_key()
    
    if not api_key:
        st.error("âš ï¸ Google API Keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Replitì˜ Secrets íƒ­ì— GOOGLE_API_KEYë¥¼ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
        st.info("API í‚¤ ì¶”ê°€ ë°©ë²•:\n1. Replit ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ 'Secrets'ë¥¼ í´ë¦­í•˜ì„¸ìš”\n2. í‚¤ ì´ë¦„ì„ 'GOOGLE_API_KEY'ë¡œ ì„¤ì •í•˜ê³  API í‚¤ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”")
        return
    
    with st.sidebar:
        st.header("ğŸ“ ë°ì´í„° ìˆ˜ì§‘")
        
        doc_count = get_document_count(api_key)
        st.metric("ì§€ì‹ ê¸°ë°˜ ë¬¸ì„œ ìˆ˜", doc_count)
        
        st.subheader("íŒŒì¼ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "PDF, DOCX, PPTX, XLSX íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=["pdf", "docx", "doc", "pptx", "ppt", "xlsx", "xls"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            if st.button("íŒŒì¼ ì²˜ë¦¬", type="primary"):
                with st.spinner("íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
                    total_chunks = 0
                    for file in uploaded_files:
                        try:
                            documents = load_file(file)
                            chunks = split_documents(documents)
                            add_documents_to_vectorstore(chunks, api_key)
                            total_chunks += len(chunks)
                            st.success(f"âœ… {file.name}: {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ë¨")
                        except Exception as e:
                            error_msg = str(e)
                            if "RESOURCE_EXHAUSTED" in error_msg or "429" in error_msg:
                                st.error(f"âŒ {file.name} API í• ë‹¹ëŸ‰ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ Google API í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ì„¸ìš”.")
                            elif "quota" in error_msg.lower():
                                st.error(f"âŒ API í• ë‹¹ëŸ‰ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                            else:
                                st.error(f"âŒ {file.name} ì²˜ë¦¬ ì˜¤ë¥˜: {error_msg}")
                    
                    if total_chunks > 0:
                        st.success(f"ì§€ì‹ ê¸°ë°˜ì— {total_chunks}ê°œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
        
        st.divider()
        
        st.subheader("URL ì¶”ê°€")
        url_input = st.text_input(
            "ì›¹ì‚¬ì´íŠ¸ URL ë˜ëŠ” YouTube ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="https://example.com ë˜ëŠ” https://youtube.com/watch?v=..."
        )
        
        is_youtube = url_input and ("youtube.com" in url_input or "youtu.be" in url_input)
        
        if not is_youtube and url_input:
            crawl_entire_site = st.checkbox("ğŸŒ ì›¹ì‚¬ì´íŠ¸ ì „ì²´ í¬ë¡¤ë§", value=True, 
                help="ì´ ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤ (ìµœëŒ€ 30í˜ì´ì§€)")
            if crawl_entire_site:
                max_pages = st.slider("ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜", 5, 30, 15)
                use_js_rendering = st.checkbox("âš¡ JavaScript ë Œë”ë§ ì‚¬ìš©", value=False,
                    help="ë™ì  ì›¹ì‚¬ì´íŠ¸ëŠ” ì¼œì„¸ìš”. ë‹¨, ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ì—¬ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                max_pages = 1
                use_js_rendering = False
        else:
            crawl_entire_site = False
            max_pages = 1
            use_js_rendering = False
        
        if url_input:
            button_text = "ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§" if crawl_entire_site else "URL ì²˜ë¦¬"
            if st.button(button_text, type="primary"):
                try:
                    if crawl_entire_site and not is_youtube:
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        def update_progress(current, total, current_url):
                            progress_bar.progress(current / total)
                            status_text.text(f"í¬ë¡¤ë§ ì¤‘ {current}/{total}: {current_url[:50]}...")
                        
                        mode_text = "JS ë Œë”ë§" if use_js_rendering else "HTTP"
                        status_text.text(f"ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘ ({mode_text} ëª¨ë“œ)...")
                        documents = crawl_website(url_input, max_pages=max_pages, progress_callback=update_progress, use_js=use_js_rendering)
                        
                        if not documents:
                            st.error("âŒ ì´ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            status_text.text(f"{len(documents)}ê°œ í˜ì´ì§€ ë°œê²¬. ì²˜ë¦¬ ì¤‘...")
                            chunks = split_documents(documents)
                            add_documents_to_vectorstore(chunks, api_key)
                            progress_bar.progress(1.0)
                            status_text.empty()
                            st.success(f"âœ… {len(documents)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§, {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ!")
                            st.rerun()
                    else:
                        with st.spinner("URLì—ì„œ ì½˜í…ì¸  ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                            documents = load_url(url_input, api_key=api_key)
                            chunks = split_documents(documents)
                            add_documents_to_vectorstore(chunks, api_key)
                            st.success(f"âœ… URLì—ì„œ {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ!")
                            st.rerun()
                except Exception as e:
                    error_msg = str(e)
                    if "RESOURCE_EXHAUSTED" in error_msg or "429" in error_msg:
                        st.error("âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ Google API í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ì„¸ìš”.")
                    elif "quota" in error_msg.lower():
                        st.error("âŒ API í• ë‹¹ëŸ‰ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                    else:
                        st.error(f"âŒ URL ì²˜ë¦¬ ì˜¤ë¥˜: {error_msg}")
        
        st.divider()
        
        st.subheader("ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰")
        
        col_year, col_month = st.columns(2)
        with col_year:
            current_year = date.today().year
            news_year = st.selectbox(
                "ì—°ë„",
                options=list(range(current_year, current_year - 5, -1)),
                index=0
            )
        with col_month:
            current_month = date.today().month
            news_month = st.selectbox(
                "ì›”",
                options=list(range(1, 13)),
                index=current_month - 1,
                format_func=lambda x: f"{x}ì›”"
            )
        
        if st.button("ğŸ” ë‰´ìŠ¤ê²€ìƒ‰", type="primary"):
            search_month_str = f"{news_year}-{news_month:02d}"
            with st.spinner(f"{news_year}ë…„ {news_month}ì›” íƒœì¬ëŒ€í•™êµ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
                try:
                    news_docs = search_google_news("íƒœì¬ëŒ€í•™êµ", search_month_str)
                    
                    if not news_docs:
                        st.warning(f"âš ï¸ {news_year}ë…„ {news_month}ì›”ì˜ íƒœì¬ëŒ€í•™êµ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        chunks = split_documents(news_docs)
                        add_documents_to_vectorstore(chunks, api_key)
                        st.success(f"âœ… {news_year}ë…„ {news_month}ì›”: {len(news_docs)}ê±´ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ {len(chunks)}ê°œ ì²­í¬ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
                except Exception as e:
                    error_msg = str(e)
                    if "RESOURCE_EXHAUSTED" in error_msg or "429" in error_msg:
                        st.error("âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                    else:
                        st.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {error_msg}")
        
        st.divider()
        
        st.subheader("ğŸ—‘ï¸ ì§€ì‹ ê¸°ë°˜ ê´€ë¦¬")
        
        type_counts = get_document_counts_by_type()
        type_labels = {
            "file": "ğŸ“„ íŒŒì¼",
            "pdf": "ğŸ“„ PDF",
            "docx": "ğŸ“ Word",
            "pptx": "ğŸ“Š PPT",
            "xlsx": "ğŸ“ˆ Excel",
            "web": "ğŸŒ ì›¹ì‚¬ì´íŠ¸",
            "website": "ğŸŒ ì›¹ì‚¬ì´íŠ¸",
            "youtube": "ğŸ¬ YouTube",
            "news": "ğŸ“° ë‰´ìŠ¤",
        }
        
        if type_counts:
            for doc_type, count in type_counts.items():
                label = type_labels.get(doc_type, f"ğŸ“ {doc_type}")
                col_label, col_btn = st.columns([3, 1])
                with col_label:
                    st.markdown(f"{label}: **{count}**ê°œ ì²­í¬")
                with col_btn:
                    if st.button("ì‚­ì œ", key=f"del_{doc_type}", type="secondary"):
                        deleted = clear_vectorstore_by_type(doc_type)
                        st.success(f"âœ… {label} ë°ì´í„° {deleted}ê°œ ì²­í¬ ì‚­ì œ ì™„ë£Œ!")
                        st.rerun()
        else:
            st.caption("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if type_counts:
            if st.button("ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”", type="secondary"):
                clear_vectorstore(api_key)
                st.session_state.messages = []
                st.session_state.sources = []
                st.success("ì§€ì‹ ê¸°ë°˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        
        if st.button("ğŸ”„ ëŒ€í™” ê¸°ë¡ ì‚­ì œ", type="secondary"):
            st.session_state.messages = []
            st.session_state.sources = []
            st.rerun()
    
    st.header("ğŸ’¬ ì§€ì‹ ê¸°ë°˜ê³¼ ëŒ€í™”í•˜ê¸°")
    
    if doc_count == 0:
        st.info("ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤! ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ URLì„ ì¶”ê°€í•˜ì—¬ ì§€ì‹ ê¸°ë°˜ì„ êµ¬ì¶•í•˜ì„¸ìš”.")
    
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            if message["role"] == "assistant" and i < len(st.session_state.sources):
                sources = st.session_state.sources[i]
                if sources:
                    with st.expander("ğŸ“– ì¶œì²˜ ë³´ê¸°"):
                        for j, source in enumerate(sources):
                            source_name = source.metadata.get("filename", source.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"))
                            doc_type = source.metadata.get("type", "unknown")
                            extra_info = ""
                            if doc_type == "pdf":
                                extra_info = f" ({source.metadata.get('page', 'N/A')}í˜ì´ì§€)"
                            elif doc_type == "pptx":
                                extra_info = f" ({source.metadata.get('slide', 'N/A')}ë²ˆ ìŠ¬ë¼ì´ë“œ)"
                            elif doc_type == "xlsx":
                                extra_info = f" (ì‹œíŠ¸: {source.metadata.get('sheet', 'N/A')})"
                            
                            st.markdown(f"**ì¶œì²˜ {j+1}: {source_name}{extra_info}**")
                            st.text(source.page_content[:500] + "..." if len(source.page_content) > 500 else source.page_content)
                            st.divider()
    
    if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            if doc_count == 0:
                response = "ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ URLì„ ì¶”ê°€í•˜ì—¬ ì§€ì‹ ê¸°ë°˜ë¥¼ êµ¬ì¶•í•´ ì£¼ì„¸ìš”."
                sources = []
            else:
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    chat_history = st.session_state.messages[:-1]
                    response, sources = get_rag_response_with_sources(prompt, chat_history, api_key)
            
            st.markdown(response)
            
            if sources:
                with st.expander("ğŸ“– ì¶œì²˜ ë³´ê¸°"):
                    for j, source in enumerate(sources):
                        source_name = source.metadata.get("filename", source.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"))
                        doc_type = source.metadata.get("type", "unknown")
                        extra_info = ""
                        if doc_type == "pdf":
                            extra_info = f" ({source.metadata.get('page', 'N/A')}í˜ì´ì§€)"
                        elif doc_type == "pptx":
                            extra_info = f" ({source.metadata.get('slide', 'N/A')}ë²ˆ ìŠ¬ë¼ì´ë“œ)"
                        elif doc_type == "xlsx":
                            extra_info = f" (ì‹œíŠ¸: {source.metadata.get('sheet', 'N/A')})"
                        
                        st.markdown(f"**ì¶œì²˜ {j+1}: {source_name}{extra_info}**")
                        st.text(source.page_content[:500] + "..." if len(source.page_content) > 500 else source.page_content)
                        st.divider()
        
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.sources.append(sources)


if __name__ == "__main__":
    main()
